{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VAE\n",
    "- a Probabilistic take on the Autoencoder\n",
    "- AE is a model which takes high dimensional input data and compresses it into a smaller representation\n",
    "\n",
    "\n",
    "- __a VAE maps the input data into the parameters of a probability dist., such as the mean and variance of a Gaussian__\n",
    "- Produces a continous, structured latent space, which is useful for image generation\n",
    "\n",
    "<img src='img/0505_1.png' width='400'>  \n",
    "\n",
    "<center> \n",
    "    Hands-On Machine Learning, 2nd edition\n",
    "</center>\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -q tensorflow-probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 20.3.3; however, version 21.1.1 is available.\n",
      "You should consider upgrading via the 'c:\\users\\hj\\anaconda3\\envs\\test_ten\\python.exe -m pip install --upgrade pip' command.\n",
      "WARNING: You are using pip version 20.3.3; however, version 21.1.1 is available.\n",
      "You should consider upgrading via the 'c:\\users\\hj\\anaconda3\\envs\\test_ten\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "# # to generate gifs\n",
    "# !pip install -q imageio\n",
    "# !pip install -q git+https://github.com/tensorflow/docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import display\n",
    "\n",
    "import glob\n",
    "import imageio\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "from tensorflow.keras import layers\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load the MNIST dataset\n",
    "- A vector of 784 integers (28*28)\n",
    "- 0-255\n",
    "\n",
    "\n",
    "- Model each pixel with a Bernoulli dist and statically binarize the data (정적으로 이진화한다)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_images, _), (test_images, _) = tf.keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(images):\n",
    "    images = images.reshape((images.shape[0], 28, 28, 1)) / 255.\n",
    "    return np.where(images > .5, 1.0, 0.0).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = preprocess_image(train_images)\n",
    "test_images = preprocess_image(test_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = 60000\n",
    "batch_size = 32\n",
    "test_size = 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Batch and shuffle the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = (tf.data.Dataset.from_tensor_slices(train_images)\n",
    "                 .shuffle(train_size).batch(batch_size))\n",
    "test_data = (tf.data.Dataset.from_tensor_slices(test_images)\n",
    "            .shuffle(test_size).batch(batch_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    - from_tensor_slices : numpy array나 list를 tensor dataset으로 변환"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Define the encoder and decoder networks\n",
    "- Use two small ConvNets for the Encoder and Decoder networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### Encoder network\n",
    "> - Defines __the approximate posterior distribution $q(z|x)$__, when $z$ is latent variable\n",
    "> \n",
    "> \n",
    "> - In this example, simply model the dist. as a diagonal Gaussian\n",
    "> - Outputs the mean and log-variance parameters of a factorized Gaussian (use log-variance for numerical stability)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### Decoder network\n",
    "> - defines __the conditional distribution of the observation $p(x|z)$__\n",
    "> - Model the latent dist.prior $p(z)$ as a unit Gaussian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### Reparameterization trick\n",
    "> <img src='img/0505_2.png' width='600'>\n",
    "> \n",
    "> <br>\n",
    "> \n",
    "> \n",
    "> - To generate a sample $z$, you can sample from the latent distribution\n",
    "> - But, by this sampling operation, backpropagation cannot flow through a random node  \n",
    "> => __Reparameterization trick!__\n",
    "> <img src='img/0505_4.png' width='300'>  \n",
    "> \n",
    "> \n",
    "> - $\\epsilon$ is a random noise, while $\\mu$ and $\\sigma$ is a fixed value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Netwrok architecture\n",
    "- Encoder : 2 Conv. layers & a FC layer\n",
    "- Decoder : 3 Conv. transpose layers & a FC layer\n",
    "\n",
    "\n",
    "- Note, it's common practice to __avoid using BN(batch normalization)__, since this may aggravate instability (stochasticity from using mini-batchs + stochasticity from sampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CVAE(tf.keras.Model):\n",
    "    '''Convolution variational autoencoder'''\n",
    "    \n",
    "    def __init__(self, latent_dim):\n",
    "        super(CVAE, self).__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.encoder = tf.keras.Sequential(\n",
    "        [\n",
    "            layers.InputLayer(input_shape=(28, 28, 1)),\n",
    "            layers.Conv2D(filters=32, kernel_size=3, strides=(2, 2), activation='relu'),\n",
    "            layers.Conv2D(64, 3, strides=(2, 2), activation='relu'),\n",
    "            layers.Flatten(),\n",
    "            layers.Dense(latent_dim + latent_dim)\n",
    "        ])\n",
    "        \n",
    "        ''' Conv2D's options\n",
    "        \n",
    "        filters : the number of output filters\n",
    "        kernel_size : size of filter\n",
    "        strides : stride (순회 간격)\n",
    "        '''\n",
    "        \n",
    "        self.decoder = tf.keras.Sequential([\n",
    "            layers.InputLayer(input_shape=(latent_dim, )),\n",
    "            layers.Dense(units=7*7*32, activation=tf.nn.relu),\n",
    "            layers.Reshape(target_shape=(7, 7, 32)),\n",
    "            layers.Conv2DTranspose(filters=64, kernel_size=3, strides=2, padding='same',\n",
    "                                  activation='relu'),\n",
    "            layers.Conv2DTranspose(32, 3, strides=2, padding='same', activation='relu'),\n",
    "            # No activation\n",
    "            layers.Conv2DTranspose(1, 3, strides=1, padding='same')\n",
    "            \n",
    "        ])\n",
    "        \n",
    "        @tf.function  # @tf.function 데코레이터 -> 자동으로 computational graph 생성\n",
    "        \n",
    "        def sample(self, eps=None):\n",
    "            if eps is None:\n",
    "                eps = tf.random.normal(shape=(100, self.latent_dim))\n",
    "            return self.decode(eps, apply_sigmoid=True)\n",
    "        \n",
    "        def encode(self, x):\n",
    "            mean, logvar = tf.split(self.encoder(x), num_or_size_splits=2, axis=1)\n",
    "            return mean, logvar\n",
    "\n",
    "            '''num_or_size_splits : 몇개로 분리할 건지'''\n",
    "        \n",
    "        def reparameterize(self, mean, logvar):\n",
    "            eps = tf.random.normal(shape=mean.shape)\n",
    "            return eps * tf.exp(logvar * .5) + mean\n",
    "        \n",
    "        def decode(self, z, apply_sigmoid=False):\n",
    "            logits = self.decoder(z)\n",
    "            if apply_sigmoid:\n",
    "                probs = tf.sigmoid(logits)\n",
    "                return probs\n",
    "            return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Define the loss function and the optimizer\n",
    "- VAEs train by __maximizing ELBO__ (evidence lower bound) on the marginal log-likeligod\n",
    "\n",
    "<img src='img/0505_7.png' width='450'>\n",
    "\n",
    "- In pratice, optimizer the single sample Monte Carlo estimate of this expectation ($z$ is sampled from $q(z|x)$)\n",
    "\n",
    "<img src='img/0505_6.png' width='250'>\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "<br>\n",
    "\n",
    "- cf) the KL term can be computed as follows\n",
    "\n",
    "<img src='img/0505_8.png' width='400'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(1e-4)\n",
    "\n",
    "def log_noraml_pdf(sample, mean, logvar, raxis=1):\n",
    "    log2pi = tf.math.log(2. * np.pi)\n",
    "    return tf.reduce_sum(\n",
    "        -.5 * ((sample - mean) ** 2. * tf.exp(-logvar) + logvar + log2pi),\n",
    "        axis=raxis)\n",
    "\n",
    "    ''' tf.reduce_sum() : 특정 차원을 제거하고 합함 (즉, 특정 차원 부분을 더함) '''\n",
    "    \n",
    "def compute_loss(model, x):\n",
    "    mean, logvar = model.encode(x)\n",
    "    z = model.reparameterize(mean, logvar)\n",
    "    x_logit = model.decode(z)\n",
    "    cross_ent = tf.nn.sigmoid_cross_entropy_with_logits(logits=x_logit, labels=x)\n",
    "    logpx_z = -tf.reduce_sum(cross_ent, axis=[1, 2, 3])\n",
    "    logpz = log_normal_pdf(z, 0., 0.)\n",
    "    logqz_x = log_normal_pdf(z, mean, logvar)\n",
    "    return -tf.reduce_mean(logpx_z + logpz - logqz_x)\n",
    "\n",
    "@tf.function\n",
    "def train_step(model, x, optimizer):\n",
    "    ''' Excutes one training steps and returns the loss.\n",
    "    \n",
    "    This function computes the loss and gradients, and uses the latter to\n",
    "    update the model's parameters.\n",
    "    '''\n",
    "    \n",
    "    with tf.GradientTape() as tape: \n",
    "        loss = compute_loss(model, x)\n",
    "        \n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    \n",
    "    '''\n",
    "    tf.GradientTape() : 컨텍스트(context) 안에서 실행된 모든 연산을 \n",
    "                        테이프(tape)에 \"기록\"\n",
    "                        \n",
    "    tape.gradient(loss, var) : var에 대하여 loss의 gradients 계산\n",
    "    .apply_gradients : 계산된 gradients를 apply\n",
    "    '''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Training\n",
    "- Start by iterating over the dataset\n",
    "- During each iteration, pass the image to the encoder to obtain a set of mean and log-var parameters of the approximate posterior $q(z|x)$\n",
    "- Then, apply the reparameterization trick to sample from $q(z|x)$\n",
    "- Finally, pass the reparameterized samples to the decoder to obtain the logits of the generative distribution $p(x|z)$\n",
    "\n",
    "#### Generating images\n",
    "- After training\n",
    "- Start by sampling a set of latent vectors from Gaussian prior dist. $p(z)$\n",
    "- The generator will then convert the latent sample z to logits of the observation, giving a dist. $p(x|z)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "# set the dimensionality of the latent space to a place for visualization later\n",
    "latent_dim = 2\n",
    "num_examples_to_generate = 10\n",
    "\n",
    "# keeping the random vector constant for generation (prediction) so\n",
    "# it will be easier to see the improvement\n",
    "random_vector_for_generation = tf.random.normal(\n",
    "    shape=[num_examples_to_generate, latent_dim])\n",
    "model = CVAE(latent_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import SVG\n",
    "from tensorflow.python.keras.utils.vis_utils import model_to_dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_2 (Conv2D)            (None, 13, 13, 32)        320       \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 6, 6, 64)          18496     \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 2304)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 4)                 9220      \n",
      "=================================================================\n",
      "Total params: 28,036\n",
      "Trainable params: 28,036\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.encoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_3 (Dense)              (None, 1568)              4704      \n",
      "_________________________________________________________________\n",
      "reshape_1 (Reshape)          (None, 7, 7, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_3 (Conv2DTr (None, 14, 14, 64)        18496     \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_4 (Conv2DTr (None, 28, 28, 32)        18464     \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_5 (Conv2DTr (None, 28, 28, 1)         289       \n",
      "=================================================================\n",
      "Total params: 41,953\n",
      "Trainable params: 41,953\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.decoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_save_images(model, epoch, test_sample):\n",
    "    mean, logvar = model.encode(test_sample)\n",
    "    z = model.reparameterize(mean, logvar)\n",
    "    predictions = model.sample(z)\n",
    "    fig = plt.figure(figsize=(4, 4))\n",
    "    \n",
    "    for i in range(predictions.shape[0]):\n",
    "        plt.subplot(4, 4, i+1)\n",
    "        plt.imshow(predictions[i, :, :, 0], cmap='gray')\n",
    "        plt.axis('off')\n",
    "        \n",
    "    # tight_layout minizes the overlat between 2 sub-plots\n",
    "    plt.savefig('img/result/CVAE_image_at_epoch_{:04d}.png'.format(epoch))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick a sample of the test set for generating output images\n",
    "assert batch_size >= num_examples_to_generate\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
